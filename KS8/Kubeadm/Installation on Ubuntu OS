Kubeadm on Debian Platform: 
The LAB is created with the help of a Master Controller - t3.medium and two worker nodes t2.micro
Process: 
Master Node: 
Create EC2 Instances, Add docker repo & install docker engine, Set-up the Cgroup=systemd, Add kubernetes repo & kubectl/kubeadm/kubelet, Init (bootstrap) kubeadm, 
Create a user & copy admin.conf in its Home Dir, Add pod netork Calico repo & install calico.yml and finally login as user to verify the cluster info.
Worker Node: Create EC2 Instances, Add docker repo & install docker engine, Set-up the Cgroup=systemd, Add kubernetes repo & kubectl/kubeadm/kubelet 
and join node to the Master.

Step 1:Basic Environment Requirement & Configuration
* 2 GB or more of RAM per machine and 2 CPUs or more
* Full network connectivity between all machines in the cluster (public or private network is fine)
* Unique hostname, MAC address
* Swap disabled in order for the kubelet to work properly
* Port Open: 
Control plane
Protocol	Direction	Port Range	Purpose	Used By
TCP	Inbound	6443	Kubernetes API server	All
TCP	Inbound	2379-2380	etcd server client API	kube-apiserver, etcd
TCP	Inbound	10250	Kubelet API	Self, Control plane
TCP	Inbound	10259	kube-scheduler	Self
TCP	Inbound	10257	kube-controller-manager	Self
Worker node(s)
Protocol	Direction	Port Range	Purpose	Used By
TCP	Inbound	10250	Kubelet API	Self, Control plane
TCP	Inbound	30000-32767	NodePort Services†	All

Login as root:
$ sudo su -
$ apt update
Set Hostname on Nodes and Hosts file:To give a unique hostname to each of your nodes
$ hostnamectl set-hostname "node-name"
(Make a host entry or DNS record to resolve the hostname for all nodes)
$ echo "ipaddress host-name" >> /etc/hosts
or
$ cat << EOF >> /etc/hosts
ipaddress host-name1
ipaddress host-name2
EOF

Disable swap : To enable the kubelet to work properly so that pods are deployed in healthy node rather than nodes maanging its resources to accomodate a pod.
$ sed -i '/swap/d' /etc/fstab
$ swapoff -a




Step 2: Install and configure prerequisites to set-up Kubernetes nodes on Linux: Bridge Network, Cgroup drivers and CRI.

1. Bridge Network: A Bridge Container Network Interface (CNI) plug-in enables all the Pods on a node to connect through virtual switch by assigning each pod
an IP address on the network and Set the net.bridge.bridge-nf-call-iptables to ‘1’ in sysctl config file to ensure that packets are properly processed by
IP tables during filtering and port forwarding.
Verify that the br_netfilter module is loaded by running lsmod | grep br_netfilter.
To load it explicitly, run sudo modprobe br_netfilter.

$ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

$ sudo modprobe overlay
$ sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
$ cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# Apply sysctl params without reboot
$ sysctl --system

2. Cgroup Driver for Docker: On Linux, control groups are used to constrain resources that are allocated to processes.
To ensure that Container runtime and kubelet cgroup drivers are same otherwise the kubelet process will fail.
Configure the Docker daemon, in particular to use systemd for the management of the container’s cgroups. When systemd is chosen as the init system for a Linux
distribution, the init process generates and consumes a root control group (cgroup) and acts as a cgroup manager.
Systemd has a tight integration with cgroups and allocates a cgroup per systemd unit.

$ cd /etc/docker and run below (If no docker dir then sudo mkdir /etc/docker)
$ cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

3.Container Runtime Interface (CRI)
Install a container runtime into each node in the cluster so that Pods can run there.
Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime (CR).
CR Types: Docker Engine, CRI-O, Containerd and Mirantis Container Runtime

CRI using Docker Engine: Docker Engine does not implement the CRI which is a requirement for a container runtime to work with Kubernetes. 
For that reason, an additional service cri-dockerd has to be installed.
* On each of your nodes, install Docker for your Linux distribution as per Install Docker Engine.
* Install cri-dockerd, following the instructions in that source code repository (https://github.com/Mirantis/cri-dockerd)

Install Docker Engine:
Supported storage drivers: On Debian/Ubuntu the Docker Engine uses the overlay2 storage driver by default while AUFS need to be configured manually. 
Uninstallation process if any previous installtion: $ sudo apt-get remove docker docker-engine docker.io containerd runc
Installation Methods: Set up Docker Respository, Download the DEB package and install it manually and Automated Convenience Scripts. Most users set up 
Docker’s repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach.

Set up the repository:
Before you install Docker Engine for the first time on a new host machine set up the Docker repository. Afterward, you can install and update Docker from the repository.
Set up the repository:

1. Update the apt package index and install packages to allow apt to use a repository over HTTPS.
$ sudo apt-get update
$ sudo apt-get install \
    ca-certificates \
    curl \
    gnupg \
    lsb-release
    
2. Add Docker’s official GPG key:
$ sudo mkdir -p /etc/apt/keyrings
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
 
3. Use the following command to set up the repository: Note its for Ubuntu specific and Not generic Debian
$ echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  
Install Docker Engine:

1. Update the apt package index, and install the latest version of Docker Engine, containerd, and Docker Compose.
$ sudo apt-get update
$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin
Note: Receiving a GPG error when running apt-get update?
Your default umask may not be set correctly, causing the public key file for the repo to not be detected.
Run the following command and then try to update your repo again: sudo chmod a+r /etc/apt/keyrings/docker.gpg

2. To install a specific version of Docker Engine, list the available versions in the repo, then select and install:
a. List the versions available in your repo:
$  apt-cache madison docker-ce
b. Install a specific version using the version string from the second column e.g. 5:20.10.16~3-0~ubuntu-jammy
$ sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io docker-compose-plugin

3. Verify that Docker Engine is installed correctly by running the hello-world image.
$ sudo docker run hello-world

Install cri-dockerd: This adapter provides a shim for Docker Engine that lets you control Docker via the Kubernetes Container Runtime Interface. 
Shim is a piece of software that resides in between a container manager (containerd, cri-o, podman) and a container runtime (runc, crun) solving 
the integration problem of these counterparts.
clone this repository in your local environment:
$ git clone https://github.com/Mirantis/cri-dockerd.git

The above step creates a local directory called cri-dockerd which you will need for the following steps.
To build this code (in a POSIX environment):
$ mkdir bin
  VERSION=$((git describe --abbrev=0 --tags | sed -e 's/v//') || echo $(cat VERSION)-$(git log -1 --pretty='%h')) 
  PRERELEASE=$(grep -q dev <<< "${VERSION}" && echo "pre" || echo "") REVISION=$(git log -1 --pretty='%h')
  go get && go build -ldflags="-X github.com/Mirantis/cri-dockerd/version.Version='$VERSION}' -X 
  github.com/Mirantis/cri-dockerd/version.PreRelease='$PRERELEASE' -X github.com/Mirantis/cri-dockerd/version.BuildTime='$BUILD_DATE' -X 
  github.com/Mirantis/cri-dockerd/version.GitCommit='$REVISION'" -o cri-dockerd

To install, on a Linux system that uses systemd, and already has Docker Engine installed so install Docker Engine before runing it:
# Run these commands as root
###Install GO###
$ wget https://storage.googleapis.com/golang/getgo/installer_linux
chmod +x ./installer_linux
./installer_linux
source ~/.bash_profile

$ cd cri-dockerd

$ mkdir bin
go get && go build -o bin/cri-dockerd
mkdir -p /usr/local/bin
install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd
cp -a packaging/systemd/* /etc/systemd/system
sed -i -e 's,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,' /etc/systemd/system/cri-docker.service
systemctl daemon-reload
systemctl enable cri-docker.service
systemctl enable --now cri-docker.socket

Note: overlay2 is the preferred storage driver for systems running Linux kernel version 4.0 or higher, or RHEL or CentOS using version 3.10.0-514 and above.
Restart Docker and enable on boot:
$ sudo systemctl enable docker
$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
$ sudo service docker status




Step 3: Add Kubernetes repository for Debian/Ubuntu:
1. Update the apt package index and install packages needed to use the Kubernetes apt repository:
$ sudo apt-get update
$ sudo apt-get install -y apt-transport-https ca-certificates curl
2. Download the Google Cloud public signing key:
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
3. Add the Kubernetes apt repository:
$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
4. Update apt package index, install kubelet, kubeadm and kubectl and pin their version:
$ sudo apt-get update
$ sudo apt-get install -y kubelet kubeadm kubectl
$ sudo apt-mark hold kubelet kubeadm kubectl
The kubelet is now restarting every few seconds, as it waits in a crashloop for kubeadm to tell it what to do.




Step 4: Initialize the Master Kubernetes: 
Three are three parts to init process - kubeadm Control Plane, Pod network and CRI Socket.
kubeadm init -  $ kubeadm init --apiserver-advertise-address=<IP Address of the Control Plane/Master Node Host or Private IP of EC2 Host>
Pod Network init: $ init --pod-network-cidr=192.168.0.0/16
CRE init: 
$ kubeadm init --apiserver-advertise-address=<IP Address> --pod-network-cidr=192.168.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock
IP Address: The IP Address of the node hosting the Control Plane or Master or the Private IP adsressof EC2 Instance hosting CP/Master Node.
Pod Network add-on: Container Network Interface (CNI) so that your Pods can communicate with each other, --pod-network-cidr=<CIDR>
CRI Socket: It will depend on the Container Runtime type 
containerd	unix:///var/run/containerd/containerd.sock
CRI-O	unix:///var/run/crio/crio.sock
Docker Engine (using cri-dockerd)	unix:///var/run/cri-dockerd.sock

e.g. we have selected EC2 Instance, Calico Netowrk for pods and Docker Engine as CR.
$ kubeadm init --apiserver-advertise-address="IP Address" --pod-network-cidr=192.168.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock

Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Verify: Cluster can not be access with root user so we need to craete a normal user.
root@master-node:~# kubectl cluster-info
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@master-node:~# kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port?



Step 5: Login with a nornal user e.g. "harness"
* Create user: $ useradd harness  
* Craeet user password: $ passwd harness  
* Add user to admin privilege group of sudoer users and add user into diffrent groups.
$ visudo 
{
# User privilege specification
root    ALL=(ALL:ALL) ALL
harness  ALL=(ALL:ALL) ALL
# Members of the admin group may gain root privileges
%admin ALL=(ALL) ALL
harness  ALL=(ALL:ALL) ALL
# Allow members of group sudo to execute any command
%sudo   ALL=(ALL:ALL) ALL
harness  ALL=(ALL:ALL) ALL
# See sudoers(5) for more information on "@include" directives:
@includedir /etc/sudoers.d
harness  ALL=(ALL) NOPASSWD:ALL
}
* Create home for user harness:  root@master-node:~# mkdir /home/harness
* Create a .kube directory inside /home/harness: root@master-node:~# cd /home/harness and root@master-node:/home/harness# mkdir .kube
* root@master-node:~# cp -i /etc/kubernetes/admin.conf /home/harness/.kube/config
* root@master-node:~# chown -R harness:harness /home/harness/.kube/config
Verify: User ownership for the config file.
root@master-node:~# ls -l /home/harness/.kube/
total 8
-rw------- 1 harness harness 5641 Aug  1 05:39 config
* root@master-node:~# su - harness  (This will change prompt as $)
$
$ pwd
/home/harness

* Verify Cluster Health:
$ kubectl cluster-info
Kubernetes control plane is running at https://172.31.14.174:6443
CoreDNS is running at https://172.31.14.174:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
$ kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
master-node   Ready    control-plane   32m   v1.24.3
$ kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true","reason":""}
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-clrmf              1/1     Running   0          20m
kube-system   coredns-6d4b75cb6d-s67t6              1/1     Running   0          20m
kube-system   etcd-master-node                      1/1     Running   0          20m
kube-system   kube-apiserver-master-node            1/1     Running   0          20m
kube-system   kube-controller-manager-master-node   1/1     Running   0          20m
kube-system   kube-proxy-xcht9                      1/1     Running   0          20m
kube-system   kube-scheduler-master-node            1/1     Running   0          20m




Step 6: Control plane node isolation: By default, your cluster will not schedule Pods on the control plane nodes for security reasons.
If you want to be able to schedule Pods on the control plane nodes, for example for a single machine Kubernetes cluster, run it from user:
$ kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master-
The output will look something like:
node "test-01" untainted
e.g 
$ kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master-
node/master-node untainted
This will remove the node-role.kubernetes.io/control-plane and node-role.kubernetes.io/master taints from any nodes that have them,
including the control plane nodes, meaning that the scheduler will then be able to schedule Pods everywhere.




Step 6: Create POD network and DNS services by adding Calico repo:
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  /docs/concepts/cluster-administration/addons/
For example create PODs using Calico Network.
$ curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml
$ kubectl apply -f calico.yaml

$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-555bc4b957-pjbkq   1/1     Running   0          6m51s
kube-system   calico-node-lsx6d                          1/1     Running   0          6m51s
kube-system   calico-typha-6c4f9c8959-hz2s5              1/1     Running   0          6m51s
kube-system   coredns-6d4b75cb6d-clrmf                   1/1     Running   0          32m
kube-system   coredns-6d4b75cb6d-s67t6                   1/1     Running   0          32m
kube-system   etcd-master-node                           1/1     Running   0          32m
kube-system   kube-apiserver-master-node                 1/1     Running   0          32m
kube-system   kube-controller-manager-master-node        1/1     Running   0          32m
kube-system   kube-proxy-xcht9                           1/1     Running   0          32m
kube-system   kube-scheduler-master-node                 1/1     Running   0          32m








Step 8. Join Nodes

