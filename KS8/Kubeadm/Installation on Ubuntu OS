Kubeadm on Debian Platform: 
The LAB is created with the help of a Master Controller - t3.medium and two worker nodes t2.micro
Process: 
Master Node: 
Create EC2 Instances, Add docker repo & install docker engine, Set-up the Cgroup=systemd, Add kubernetes repo & kubectl/kubeadm/kubelet, Init (bootstrap) kubeadm, 
Create a user & copy admin.conf in its Home Dir, Add pod netork Calico repo & install calico.yml and finally login as user to verify the cluster info.
Worker Node: Create EC2 Instances, Add docker repo & install docker engine, Set-up the Cgroup=systemd, Add kubernetes repo & kubectl/kubeadm/kubelet 
and join node to the Master.

Step 1:Basic Configuration and Installing Pre-requisites
$ sudo su -
$ apt update
Set Hostname on Nodes and Hosts file:To give a unique hostname to each of your nodes
$ hostnamectl set-hostname "node-name"
(Make a host entry or DNS record to resolve the hostname for all nodes)
$ echo "ipaddress host-name" >> /etc/hosts
or
$ cat << EOF >> /etc/hosts
ipaddress host-name1
ipaddress host-name2
EOF

Disable Firewall: (The nodes, containers, and pods need to be able to communicate across the cluster to perform their functions. 
Firewalld is enabled in CentOS by default on the front-end.Add the following ports by entering the listed commands)
On the Master Node enter:
sudo firewall-cmd --permanent --add-port=6443/tcp
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=10251/tcp
sudo firewall-cmd --permanent --add-port=10252/tcp
sudo firewall-cmd --permanent --add-port=10255/tcp
sudo firewall-cmd --reload
Enter the following commands on each worker node:
sudo firewall-cmd --permanent --add-port=10251/tcp
sudo firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --reload
or
systemctl disable firewalld
systemctl stop firewalld

Configuring a bridge network and Update Iptables Settings: A Bridge Container Network Interface (CNI) plug-in enables all the Pods on a node to connect 
through virtual switch by assigning each pod an IP address on the network and Set the net.bridge.bridge-nf-call-iptables to ‘1’ in sysctl config file 
to ensure that packets are properly processed by IP tables during filtering and port forwarding.
$ cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
$ sysctl --system

Disable SELinux: The containers need to access the host filesystem: Security-Enhanced Linux (SELinux) is a Linux kernel security module that provides a 
mechanism for supporting access control security policies with default restricted all so SELinux needs to be set to permissive mode, which effectively 
disables its security functions.
$ setenforce 0
$ sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux
or
$ sudo setenforce 0
$ sudo sed -i ‘s/^SELINUX=enforcing$/SELINUX=permissive/’ /etc/selinux/config

Disable swap : To enable the kubelet to work properly so that pods are deployed in healthy node rather than nodes maanging its resources to accomodate a pod.
$ sed -i '/swap/d' /etc/fstab
$ swapoff -a


Step2: Enabling Container Runtime Interface (CRI)
Installing Container Runtimes (CR) - To run containers in Pods, Kubernetes uses a container runtime and Kubernetes uses the Container Runtime Interface (CRI)
to interface with your chosen container runtime.
Container Runtime Types: Docker Engine, CRI-O, Containerd and Mirantis Container Runtime

CRI using Docker Engine:
* On each of your nodes, install Docker for your Linux distribution as per Install Docker Engine.
* Install cri-dockerd, following the instructions in that source code repository.

Install Docker Engine:
Supported storage drivers: On Debian/Ubuntu the Docker Engine uses the overlay2 storage driver by default while AUFS need to be configured manually. 
Uninstallation process if any previous installtion: $ sudo apt-get remove docker docker-engine docker.io containerd runc
Installation Methods: Set up Docker Respository, Download the DEB package and install it manually and Automated Convenience Scripts. Most users set up 
Docker’s repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach.
Install using Set up Docker Respository:
Before you install Docker Engine for the first time on a new host machine set up the Docker repository. Afterward, you can install and update Docker from the repository.
Set up the repository:

1. Update the apt package index and install packages to allow apt to use a repository over HTTPS.
$ sudo apt-get update
$ sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release
or
< $ sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release >
2. Add Docker’s official GPG key:
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
3. Set up the stable repository:
$ echo \
  "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
or
< echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null >

Docker Engine Installation process:

1. Update the apt package index and install the latest version of Docker Engine and containerd or install a specific version.
$ sudo apt-get update
$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin
Note: Receiving a GPG error when running apt-get update?
Your default umask may not be set correctly, causing the public key file for the repo to not be detected.
Run the following command and then try to update your repo again: sudo chmod a+r /etc/apt/keyrings/docker.gpg
2. To install a specific version of Docker Engine, list the available versions in the repo, then select and install:
a. List the versions available in your repo:
$  apt-cache madison docker-ce
b. Install a specific version using the version string from the second column e.g. 5:20.10.16~3-0~ubuntu-jammy
$ sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io docker-compose-plugin
3. Verify that Docker Engine is installed correctly by running the hello-world image.
$ sudo docker run hello-world

Install cri-dockerd:
clone this repository in your local environment:
$ git clone https://github.com/Mirantis/cri-dockerd.git
The above step creates a local directory called cri-dockerd which you will need for the following steps.
To build this code (in a POSIX environment):
$ mkdir bin
  VERSION=$((git describe --abbrev=0 --tags | sed -e 's/v//') || echo $(cat VERSION)-$(git log -1 --pretty='%h')) 
  PRERELEASE=$(grep -q dev <<< "${VERSION}" && echo "pre" || echo "") REVISION=$(git log -1 --pretty='%h')
  go get && go build -ldflags="-X github.com/Mirantis/cri-dockerd/version.Version='$VERSION}' -X 
  github.com/Mirantis/cri-dockerd/version.PreRelease='$PRERELEASE' -X github.com/Mirantis/cri-dockerd/version.BuildTime='$BUILD_DATE' -X 
  github.com/Mirantis/cri-dockerd/version.GitCommit='$REVISION'" -o cri-dockerd
To install, on a Linux system that uses systemd, and already has Docker Engine installed so install Docker Engine before runing it:
# Run these commands as root
###Install GO###
wget https://storage.googleapis.com/golang/getgo/installer_linux
chmod +x ./installer_linux
./installer_linux
source ~/.bash_profile

cd cri-dockerd
mkdir bin
go get && go build -o bin/cri-dockerd
mkdir -p /usr/local/bin
install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd
cp -a packaging/systemd/* /etc/systemd/system
sed -i -e 's,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,' /etc/systemd/system/cri-docker.service
systemctl daemon-reload
systemctl enable cri-docker.service
systemctl enable --now cri-docker.socket


Step 3: Set up the Cgroup Driver for Docker: To ensure that Container runtime and kubelet cgroup drivers are same otherwise the kubelet process will fail.
Configure the Docker daemon, in particular to use systemd for the management of the container’s cgroups.

$ cd /etc/docker and run below (If no docker dir then sudo mkdir /etc/docker)
$ cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF

Note: overlay2 is the preferred storage driver for systems running Linux kernel version 4.0 or higher, or RHEL or CentOS using version 3.10.0-514 and above.
Restart Docker and enable on boot:
sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker


Step 4: Add Kubernetes repository for Debian/Ubuntu:
1. Update the apt package index and install packages needed to use the Kubernetes apt repository:
$ sudo apt-get update
$ sudo apt-get install -y apt-transport-https ca-certificates curl
2. Download the Google Cloud public signing key:
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
3. Add the Kubernetes apt repository:
$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
4. Update apt package index, install kubelet, kubeadm and kubectl and pin their version:
$ sudo apt-get update
$ sudo apt-get install -y kubelet kubeadm kubectl
$ sudo apt-mark hold kubelet kubeadm kubectl
The kubelet is now restarting every few seconds, as it waits in a crashloop for kubeadm to tell it what to do.


Step 5: Initialize the Master Kubernetes:
$ kubeadm init --apiserver-advertise-address="IP Address of Master Node (Private of EC2)" --pod-network-cidr=192.168.0.0/16
( Read the instruction which says that run the kubectl or access the cluster using a normal user and root is not recommended)

Note: Multiple CRI sockets.
Error: "Kubespray fails with "Found multiple CRI sockets, please use --cri-socket to select one"
Solution: Select a CRI e.g. Docker in our case.
$ kubeadm init --apiserver-advertise-address="IP Address" --pod-network-cidr=192.168.0.0/16 --cri-socket=unix:///var/run/cri-dockerd.sock

Your Kubernetes control-plane has initialized successfully!
o start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Login with a nornal user e.g. "harness"
1. Create user: $ useradd harness  
2. Craeet user password: $ passwd harness  
3. Add user to admin privilege group of sudoer users and add user into diffrent groups.
$ visudo 
{
# User privilege specification
root    ALL=(ALL:ALL) ALL
harness  ALL=(ALL:ALL) ALL
# Members of the admin group may gain root privileges
%admin ALL=(ALL) ALL
harness  ALL=(ALL:ALL) ALL
# Allow members of group sudo to execute any command
%sudo   ALL=(ALL:ALL) ALL
harness  ALL=(ALL:ALL) ALL
# See sudoers(5) for more information on "@include" directives:
@includedir /etc/sudoers.d
harness  ALL=(ALL) NOPASSWD:ALL
}
4. Create home for user harness:  root@master-node:~# mkdir /home/harness
5. Create a .kube directory inside /home/harness:  root@master-node:~# mkdir -p $HOME/harness/.kube
(Need to experiment whther 5th step is enough to create home for new user as well as dir .kube directory to avoid step 4th)
5. root@master-node:~# cp -i /etc/kubernetes/admin.conf /home/harness/.kube/config
6. root@master-node:~# chown -R harness:harness /home/harness/.kube/config
7. root@master-node:~# su - harness  (This will change prompt as $)
$
$ pwd
/home/harness
8. $ kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
master-node   Ready    control-plane   32m   v1.24.3


Step 6: Create POD network and DNS services by adding Calico repo:
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  /docs/concepts/cluster-administration/addons/
For example create PODs using Calico Network.
$ curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico.yaml
$ kubectl apply -f calico.yaml


Join Nodes:

