What is Kubeadm ?
Kubeadm is a tool built to provide best-practice "fast paths" for creating Kubernetes clusters. It performs the actions necessary to get a minimum viable, 
secure cluster up and running in a user friendly way. Kubeadm's scope is limited to the local node filesystem and the Kubernetes API, and it is intended to be a 
composable building block of higher level tools.

Common Kubeadm Commands:
kubeadm init to bootstrap the initial Kubernetes control-plane node.
kubeadm join to bootstrap a Kubernetes worker node or an additional control plane node, and join it to the cluster.
kubeadm upgrade to upgrade a Kubernetes cluster to a newer version.
kubeadm reset to revert any changes made to this host by kubeadm init or kubeadm join.

This documentation guides you in setting up a cluster using CentOS with one master node and two worker nodes.

Prerequisites:
# System Requirements:
Master: t2.medium (2 CPUs and 2GB Memory)
Worker Nodes: t2.micro
Ports:
# Control-Plane Node
Protocol	Port Number	Description
TCP	6443	Kubernetes API Server
TCP	2379-2380	etcd server client API
TCP	10250	Kubelet API
TCP	10251	kube-scheduler
TCP	10252	kube-controller-manager
TCP	10255	Read-only Kubelet API
# Worker node:
Protocol	Port Number	Description
TCP	10250	Kubelet API
TCP	10255	Read-only Kubelet API
TCP	30000-32767	NodePort Services
# CNI ports on both control-plane and worker nodes (CNI specific ports are only required to be opened when that particular CNI plugin is used)
Protocol	Port Number	Description
TCP	179	Calico BGP network
TCP	9099	Calico felix (health check)
UDP	8285	Flannel
UDP	8472	Flannel
TCP	6781-6784	Weave Net
UDP	6783-6784	Weave Net

Perform all the commands as root user unless otherwise specified on Master Nodes and all the Worker Nodes.

Step 1. Docker Instalaltion: 
( To Install Docker on CentOS 7 With Yum we should use Docker repository to ensure upadted docker version is installed.)
# Install Docker CE
yum install yum-utils device-mapper-persistent-data lvm2
# Add Docker repository.
yum-config-manager \
  --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo
# Install Docker CE
yum update && yum install docker-ce-18.06.2.ce
# Create /etc/docker directory.
mkdir /etc/docker
# Configure the Docker daemon
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF
mkdir -p /etc/systemd/system/docker.service.d
# Restart Docker
systemctl daemon-reload
systemctl restart docker
systemctl enable docker
systemctl status docker

Step 2. Adding Kubernetes Packages for CentOS:
(Kubernetes packages are not available from official CentOS 7 repositories. This step needs to be performed on the Master Node, and each Worker Node you plan on utilizing
for your container setup. Enter the following command to retrieve the Kubernetes repositories.)
cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

Step 3: Install kubelet, kubeadm, and kubectl
(These 3 basic packages are required to be able to use Kubernetes. Install the following package(s) on each node:)
sudo yum install -y kubelet kubeadm kubectl
(Enable and Start kubelet service)
systemctl enable kubelet
systemctl start kubelet

Step 4: Set Hostname on Nodes and Hosts file:
(To give a unique hostname to each of your nodes)
hostnamectl set-hostname ndoe-name
(Make a host entry or DNS record to resolve the hostname for all nodes)
echo "ipaddress host-name" >> /etc/hosts
or
cat << EOF >> /etc/hosts
ipaddress host-name1
ipaddress host-name2
EOF

Step 5: Disable Firewall:
(The nodes, containers, and pods need to be able to communicate across the cluster to perform their functions. Firewalld is enabled in CentOS by default on the front-end.
Add the following ports by entering the listed commands)
On the Master Node enter:
sudo firewall-cmd --permanent --add-port=6443/tcp
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
sudo firewall-cmd --permanent --add-port=10250/tcp
sudo firewall-cmd --permanent --add-port=10251/tcp
sudo firewall-cmd --permanent --add-port=10252/tcp
sudo firewall-cmd --permanent --add-port=10255/tcp
sudo firewall-cmd --reload
Enter the following commands on each worker node:
sudo firewall-cmd --permanent --add-port=10251/tcp
sudo firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --reload
or
systemctl disable firewalld
systemctl stop firewalld

Step 6: Configuring a bridge network and Update Iptables Settings:
(A Bridge Container Network Interface (CNI) plug-in enables all the Pods on a node to connect through virtual switch by assigning each pod an IP address on the network 
and Set the net.bridge.bridge-nf-call-iptables to ‘1’ in sysctl config file to ensure that packets are properly processed by IP tables during filtering and port forwarding)
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

Step 7: Disable SELinux: 
(The containers need to access the host filesystem: Security-Enhanced Linux (SELinux) is a Linux kernel security module that provides a mechanism for supporting access 
control security policies with default restricted all so SELinux needs to be set to permissive mode, which effectively disables its security functions)
setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux
or
sudo setenforce 0
sudo sed -i ‘s/^SELINUX=enforcing$/SELINUX=permissive/’ /etc/selinux/config

Step 8: Disable swap
(To enable the kubelet to work properly so that pods are deployed in healthy node rather than nodes maanging its resources to accomodate a pod)
sed -i '/swap/d' /etc/fstab
swapoff -a

Node Specific Configurations:

Master Node: Initialize Kubernetes Cluster

kubeadm init --apiserver-advertise-address=<MasterServerIP> --pod-network-cidr=192.168.0.0/16
Create a user for kubernetes administration and copy kube config file.
To be able to use kubectl command to connect and interact with the cluster, the user needs kube config file.
In this case, we are creating a user called kubeadmin

useradd kubeadmin
passwd kubeadmin
mkdir /home/kubeadmin/.kube
cp /etc/kubernetes/admin.conf /home/kubeadmin/.kube/config
chown -R kubeadmin:kubeadmin /home/kubeadmin/.kube
chmod -R 755 /home/kubeadmin/.kube
su - kubeadmin


Set Up Pod Network: A Pod Network allows nodes within the cluster to communicate. There are several available Kubernetes networking options.
Deploy Calico network as a kubeadmin user.
sudo su - kubeadmin 
kubectl create -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml

Deploy Flannel netwoek: edit the firewall rules to allow traffic for the flannel default port 8285.
sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

$ kubectl get pods -o wide --all-namespaces
$ kubectl get pods --all-namespaces

Cluster join command:
kubeadm token create --print-join-command

On Worker Node:
#Add worker nodes to cluster by using the output from kubeadm token create command in previous step from the master server and run here.

#Verifying the cluster To Get Nodes status
kubectl get nodes
sudo kubectl get pods --all-namespaces

#To Get component status
kubectl get cs


How to access Cluster from Local System/Laptop:
Admin user access: 
(Copy the administrator kubeconfig file from your control-plane node to your workstation lor laptop, admin.conf file gives the user superuser privileges over the cluste
$ scp root@<control-plane-host>:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf get nodes
Normal user access: 
(It's recommended to generate an unique credential to which you grant privileges. Run kubeadm alpha kubeconfig user --client-name <CN> command. This command will print 
out a KubeConfig file to STDOUT which you should save to a file and distribute to your user. After that, grant privileges by using kubectl create (cluster)rolebinding.)

(Optional) Proxying API Server to localhost: Use kubectl proxyto connect to the API Server from outside the cluster.
$ scp root@<control-plane-host>:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf proxy
Now access the API Server locally at http://localhost:8001/api/v1

Clean up:
$ kubectl config delete-cluster
Remove the node: First Drain and reset the state before removing the node
$ kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
$ kubeadm reset
# The reset process does not reset or clean up iptables rules or IPVS tables and if you wish to reset iptables, you must do so manually 
$ iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
$ ipvsadm -C    (To reset IPVS tables)
$ kubectl delete node <node name>
# Clean up the control plane:
$ kubeadm reset
